---
title: "Assignment 1"
output: html_notebook
---

First things first, load the libraries we need for the analysis. `Tidyverse` for dplyr and others, `lubridate` for date manipulation and `factoextra` for cluster visualization
```{r}
library(tidyverse)
library(lubridate)
library(factoextra)
```

# Importing and cleaning data
Let's start by importing the raw data into R. I am also going to import the people data for use later on.

```{r}
cities <- fread("data/cities_df.csv", header = TRUE, check.names = TRUE, data.table = FALSE)
trips <- fread("data/trips_df.csv", header = TRUE, check.names = TRUE, data.table = FALSE)
people <- read_tsv("data/people_df.tsv")
```

## City data
First, we'll have a look at the data we got for the cities.

```{r}
cities %>% head()
```

The column names are capitalized. Let's convert them to all lowercase to make it easier to work with. 

```{r}
colnames(cities) <- tolower(colnames(cities))
cities <- cities[,!duplicated(colnames(cities))]
```

That's better. By doing `duplicated(cities$place_slug)`, we see that there's no duplicates of any of the cities, so removing duplicates is unnecessary.
The  `read_tsv` function created a new column with some rownames for us, in this case the X1 column with values ranging from 1-731. It would however make more sense to make the place_slug our rowname, since that indicates which city each row is representing. Let's do that.

```{r}
rownames(cities) <- cities[,"place_slug"]
```


Let's convert the ratings to factors, thus enabling sorting of these. I am manually setting the factor levels such that it is correctly sorted from bad to great.

```{r}
factor_levels <- c("bad", "okay", "good", "great")
cities <- cities %>% mutate(a.c.or.heating = factor(a.c.or.heating, levels = factor_levels),
                            adult.nightlife = factor(adult.nightlife, levels = factor_levels),
                            air.quality = factor(air.quality, levels = factor_levels),
                            cost.of.living = factor(cost.of.living, levels = factor_levels),
                            english.speaking = factor(english.speaking, levels = factor_levels),
                            female.friendly = factor(female.friendly, levels = factor_levels),
                            free.wifi.in.city = factor(free.wifi.in.city, levels = factor_levels),
                            freedom.of.speech = factor(freedom.of.speech, levels = factor_levels),
                            friendly.to.foreigners = factor(friendly.to.foreigners, levels = factor_levels),
                            fun = factor(fun, levels = factor_levels),
                            happiness = factor(happiness, levels = factor_levels),
                            healthcare = factor(healthcare, levels = factor_levels),
                            lgbt.friendly = factor(lgbt.friendly, levels = factor_levels),
                            nightlife = factor(nightlife, levels = factor_levels),
                            peace = factor(peace, levels = factor_levels),
                            places.to.work.from = factor(places.to.work.from, levels = factor_levels),
                            quality.of.life = factor(quality.of.life, levels = factor_levels),
                            racial.tolerance = factor(racial.tolerance, levels = factor_levels),
                            safety = factor(safety, levels = factor_levels),
                            startup.score = factor(startup.score, levels = factor_levels),
                            traffic.safety = factor(traffic.safety, levels = factor_levels),
                            walkability = factor(walkability, levels = factor_levels))
```

I am also going to convert the Average trip length to a numeric value by first removing the "days" part from the variable using a regular expression, and then converting the resulting string to a numeric value. This enables us to do some analysis with it.

```{r}
cities <- cities %>% mutate(avg.trip.length = as.numeric(str_extract(avg.trip.length, "[0-9]+")))
```

Let's also convert the population and internet speed to a numeric value, more specifically a double like the above. The regular expression below takes everything that isn't a numeric value and replaces it with nothing.

```{r}
cities <- cities %>% mutate(population = as.numeric(gsub("[^0-9+]", "", population)),
                            internet.speed = as.numeric(gsub("[^0-9+]", "", internet.speed)))
head(cities$population)
```

Let's use the same trick for other numeric values that we want to use later. Here, I use a regular expression to get the first number after the dollar-sign, and then remove the dollar-sign afterwards. 

```{r}
cities <- cities %>% mutate(beer = as.numeric(gsub("\\$", "", str_extract(beer, "\\$[0-9,.]+"))),
                            airbnb = as.numeric(gsub("\\$", "", str_extract(airbnb, "\\$[0-9,.]+"))),
                            coffee = as.numeric(gsub("\\$", "", str_extract(coffee, "\\$[0-9,.]+"))),
                            hotel = as.numeric(gsub("\\$", "", str_extract(hotel, "\\$[0-9,.]+"))))
head(cities)
```


## Trip data

First up, let's have a look at the trip data.

```{r}
trips %>% head()
```

As we can see, the trip data has the same place_slug and country_slug column as the cities data. This makes for a great key when we are joining our data. Great!

Since the date_end and date_start columns contain dates, let's convert them to a date datatype.

```{r}
trips <- trips %>% drop_na(date_end, date_start) %>% mutate(date_end = ymd(date_end), date_start = ymd(date_start))
trips <- trips %>% drop_na(date_end, date_start)
```

After the conversion, I drop all NA's from the two columns. I do this because some of the dates failed to parse, and removing NA's removes these.

## People data

```{r}
people %>% head()
```

Not much to do here, looks good! I could convert the `work` and `like` columns to vectors, but I choose not to do that since I use a different approach for getting these later on.

# Joining and Exploring the data

## Trip lengths

The city data has lots of useful information, among this the average trip length. In my data cleanup, I converted this to a numeric value to enable analysis such as the following. 

Everyone loves a drink or two, but is that also reflected in the average trip length to the different cities? Let's find out by grouping the data by the nightlife factor and getting the mean of the average trip lengths.

```{r}
fun_avg_trip <- cities %>% group_by(adult.nightlife) %>% filter(!is.na(avg.trip.length), !is.na(adult.nightlife)) %>% summarise(mean = mean(avg.trip.length))
ggplot(fun_avg_trip, aes(x = adult.nightlife, y = mean)) + geom_bar(stat = "identity")
```

It seems that is indeed the case! The average trip length is longer where the nightlife is great.

## Think of the environment!

I wonder how the air quality differs across regions. To do this, I first group the data by air quality and region, and then use the `tally()` function to count the number of occurrences of air quality for each region. Lastly, I add the column `scale`, which is the scaled count to more correctly represent the data. I do this since there are many more cities in some regions than others.

I then generate a plot of the results, using `position = "dodge"` to have the bars grouped together instead of overlapping.

```{r}
region.air.quality <- cities %>% group_by(air.quality, region) %>% drop_na() %>% tally() %>% mutate(scale = scale(n, center = FALSE))
ggplot(region.air.quality, aes(fill = region, x = air.quality, y = scale)) + geom_bar(position = "dodge", stat = "identity")
```

As expected, the air quality in the Asian countries is generally worse. The air quality in Europe is very good overall, with the Americas coming in second.

## Adult nightlife, happiness, fun and beer prices!

I wonder if there's any patterns between the adult nightlife, happiness, fun and beer prices in different countries. One would think so!

First off, I select only the data I am interested in exploring, and then convert the factors to numeric values so they are workable. I also remove any NA's and duplicate rows.

```{r}
cities.clustering <- cities %>% select(adult.nightlife, happiness, fun, beer, place_slug, country)
cities.clustering <- cities.clustering %>% mutate(adult.nightlife = as.numeric(adult.nightlife),
                                                  happiness = as.numeric(happiness),
                                                  fun = as.numeric(fun))
cities.clustering <- cities.clustering %>% drop_na()
cities.clustering <- cities.clustering %>% distinct(country, .keep_all = TRUE)
```

Using the so called Elbow method, we'll try to determine the optimal number of clusters for our clustering.

```{r}
fviz_nbclust(scale(cities.clustering %>% select(-place_slug, -country)), kmeans, method = "wss")
```

Since the peak starts at 4-6, I will make a decision and go with 6 as my k.

Next, I set the rownames since fviz uses those for visualization. I then create a K-Means cluster, making sure to scale it to account for different observation counts in the data (for comparability's sake).

```{r}
rownames(cities.clustering) <- cities.clustering %>% pull(country)
km.cities.clustering <- kmeans(cities.clustering %>% select(-place_slug, -country) %>% scale(), centers = 6)
fviz_cluster(km.cities.clustering, data = cities.clustering %>% select(-place_slug, -country) %>% scale())
```

This doesn't tell us much, but we can see that the results are very clustered, so no real spread. We also see that there's a clear outlier in South Korea, where beer is very expensive. Let's take a closer look at the data.

```{r}
cities.clustering %>% bind_cols(cluster = km.cities.clustering$cluster) %>% select(-place_slug, -country) %>% group_by(cluster) %>% mutate(n = n()) %>% summarise_all(funs(mean))
```

Interestingly, the 2nd cluster has the second highest beer prices, but also scores relatively high in all other parameters. Maybe expensive beer is better after all?

By joining our clustered dataframe with the trips data, we can see how many people visited the countries in the different clusters. First up, we left join the trips with the cities data, meaning that we take everything in the trips data where `place_slug` matches the `place_slug` in the clustered cities data.

```{r}
cities.clustering <- cities.clustering %>% bind_cols(cluster = km.cities.clustering$cluster)
trips.clustering <- trips %>% left_join(cities.clustering %>% select(place_slug, cluster), by = "place_slug")
```

Let's group the trips by cluster, and count the number of rows.

```{r}
trips.clustering %>% group_by(cluster) %>% count()
```

Alright, a lot of people visited the second cluster. My assumption is that the second cluster consists of primarily European countries. Let's check if that is correct.

```{r}
cities.clustering.region <- cities %>% left_join(cities.clustering %>% select(place_slug, cluster), by = "place_slug")
cities.clustering.region %>% group_by(cluster, region) %>% summarise(count = n())
```

Alright, seems like that was indeed the case. However, there were also countries from Latin America and the Middle East. Interesting.

Please not that the above could have been also accomplished without using join, by simply including the region when we created the clustering. However, this nicely demonstrates what joins can do.

## Internet speed and programmers

Let's make use of the people data! Programmers need internet to work, I wonder if there's a correlation between the internet speed and the places they visit the most. Let's find out!

In this case, I join the trips data with the people data, using the username as the key. I then filter the data, including only people with "Web Dev" and/or "Software Dev" as their profession.

```{r}
trips.work <- trips %>% left_join(people %>% select(work, like, username), by = "username")
trips.work.developers <- trips.work %>% filter(grepl("Web Dev", work) | grepl("Software Dev", work))
```

Now we have all the trips of people who are presumably programmers. Let's join that data with the cities data to get more information about the places they visited. Here, I join using the places column as my key, since the place_slug for some reason has some cities marked with only their respective country.
Lastly I summarize the data, counting the amount of trips to each location.

```{r}
cities.developers <- trips.work.developers %>% left_join(cities %>% select(place, region, internet.speed), by = "place")
cities.developers.trips <- cities.developers %>% group_by(place, region, internet.speed) %>% drop_na() %>% summarise(count = n())
#cities.developers["scale"] <- scale(cities.developers$count, center = FALSE)
```

Let's plot the data to see whats going on!

```{r}
ggplot(cities.developers.trips, aes(x = internet.speed, y = count, fill = region)) + geom_bar(stat = "identity") + scale_x_log10()
```

Hmm, looks like they don't necessarily prefer high speeds. Maybe they are going on vacation to get away from work, makes sense to me. Let's see if we can plot it in a different way, this time by region instead. Here I calculate the mean of the internet speed of the different regions, and use the color of the bars to indicate the amount of trips to each region. I also sort the plot by number of trips.

```{r}
cities.developers.trips <- cities.developers %>% group_by(region) %>% drop_na() %>% summarise(internet.speed.mean = mean(internet.speed), count = n())
ggplot(cities.developers.trips, aes(x = reorder(region, count), y = internet.speed.mean, fill = count)) + geom_bar(stat = "identity")
```

A large number of the trips are to regions where the internet is good, but then again these regions are also some of the biggest, so that makes sense.

## Programmers and their precious coffee!

Programmers like coffee, but do they organize their trips to get the cheapest of it? Let's do like before and join the trip data with the city data, this time also selecting the coffee price data.

```{r}
cities.developers <- trips.work.developers %>% left_join(cities %>% select(place, region, coffee), by = "place")
cities.developers.trips <- cities.developers %>% group_by(place, region, coffee) %>% drop_na() %>% summarise(count = n())
cities.developers.trips
```

Looks good. However, we clearly need to filter out some results, since there are outliers in the data (like a $150 cup of coffee). We do this by only selecting the coffee prices under $50. I also round the prices to limit the amount of data in the plot.

```{r}
cities.developers.trips <- cities.developers.trips %>% filter(coffee < 50)
ggplot(cities.developers.trips, aes(x = round(coffee), y = count, fill = region)) + geom_bar(stat = "identity")
```

It seems like the sweet spot is 2 to 3 dollars for a cup of coffee! Of cause, it might also just be a coincidence. Since there is no big outlier, I will assume that programmers don't specifically go after cheap coffee when they travel.

## Trips by date

Let's have a look at the dates of the trips. It would be interesting to see when people start their trips. I am going to specifically look at trips last year, just for demonstration.

```{r}
trips_bydate <- trips %>% filter(date_start >= ymd(20170101) & date_start <= ymd(20180101)) %>% group_by(date_start = date(date_start)) %>% summarise(count = n())
trips_bydate
```

Let's plot the results!

```{r}
ggplot(trips_bydate, aes(x = date_start, y = count)) + geom_line()
```

The amount of trips are pretty consistent it seems. Let's try taking the average trips of each month and plotting them.

```{r}
trips_bydate <- trips_bydate %>% group_by(month = month(date_start)) %>% mutate(mean = mean(count))
trips_bydate
ggplot(trips_bydate, aes(x = month, y = mean)) + geom_line()
```

There's definitely a peak in the total trips in the start and end of the summer months.

We can also try plotting trips by region, seeing if there are any standouts. Here I join with the city data again to get the region.

```{r}
trips_bydate <- trips %>% left_join(cities %>% select(place, region), by = "place") %>% drop_na()
trips_bydate <- trips_bydate %>% filter(date_start >= ymd(20170101) & date_start <= ymd(20180101)) %>% group_by(date_start = date(date_start), region) %>% summarise(count = n())
trips_bydate
ggplot(trips_bydate, aes(x = date_start, y = count, color = region)) + geom_line()
```

A bit cluttered, but Europe is the most popular destination it seems. Some large peaks for Asia and North America too.

How does this look in the summer months?

```{r}
trips_bydate <- trips_bydate %>% filter(date_start >= ymd(20170601) & date_start < ymd(20170801))
ggplot(trips_bydate, aes(x = date_start, y = count, color = region)) + geom_line()
```

Pretty steady trip rate, but the middle of the months are peaks for starting trips. Interesting.